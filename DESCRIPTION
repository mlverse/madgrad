Package: madgrad
Title: 'MADGRAD' Method for Stochastic Optimization
Version: 0.1.0.9000
Authors@R: c(
    person("Daniel", "Falbel", email = "daniel@rstudio.com", role = c("aut", "cre", "cph")),
    person(family = "RStudio", role = c("cph")),
    person(family = "MADGRAD original implementation authors.", role = c("cph"))
    )
Description: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic 
  Optimization algorithm. MADGRAD is a 'best-of-both-worlds' optimizer with the 
  generalization performance of stochastic gradient descent and at least as fast 
  convergence as that of Adam, often faster. A drop-in optim_madgrad() implementation
  is provided based on Defazio et al (2020) <arxiv:2101.11075>.
License: MIT + file LICENSE
Encoding: UTF-8
Roxygen: list(markdown = TRUE)
RoxygenNote: 7.1.1
Imports: 
    torch (>= 0.3.0),
    rlang
Suggests: 
    testthat (>= 3.0.0)
Config/testthat/edition: 3
