Package: madgrad
Title: Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization
Version: 0.0.0.9000
Authors@R: c(
    person("Daniel", "Falbel", email = "daniel@rstudio.com", role = c("aut", "cre", "cph")),
    person(family = "RStudio", role = c("cph"))
    )
Description: A best-of-both-worlds optimizer with the generalization performance 
  of SGD and at least as fast convergence as that of Adam, often faster. A drop-in 
  'optim_madgrad' implementation by Defazio et al (2020) <arxiv:https://arxiv.org/abs/2101.11075>.
License: MIT + file LICENSE
Encoding: UTF-8
LazyData: true
Roxygen: list(markdown = TRUE)
RoxygenNote: 7.1.1
Imports: 
    torch,
    rlang
Remotes:
    mlverse/torch
Suggests: 
    testthat (>= 3.0.0)
Config/testthat/edition: 3
